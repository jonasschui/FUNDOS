{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in dependent libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.classify\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from nltk import *\n",
    "import collections, itertools\n",
    "from nltk.metrics import precision, recall\n",
    "#from featx import bag_of_words, high_information_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral', 'negative', 'positive']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the training data\n",
    "train_df = pd.read_csv(\"data/HCR/train/orig/HCR_formatted_train.csv\")\n",
    "# retrieve the possible labels from the training data\n",
    "categories = list(set(train_df['sentiment']))\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mannaully coppied the featx bag_of_words and high_information_words module since featx package is not available for conda\n",
    "def high_information_words(labelled_words, score_fn=BigramAssocMeasures.chi_sq, min_score=5):\n",
    "    word_fd = FreqDist()\n",
    "    label_word_fd = ConditionalFreqDist()\n",
    "\n",
    "    for label, words in labelled_words:\n",
    "        for word in words:\n",
    "            word_fd[word] += 1\n",
    "            label_word_fd[label][word] += 1\n",
    "\n",
    "    n_xx = label_word_fd.N()\n",
    "    high_info_words = set()\n",
    "\n",
    "    for label in label_word_fd.conditions():\n",
    "        n_xi = label_word_fd[label].N()\n",
    "        word_scores = collections.defaultdict(int)\n",
    "\n",
    "        for word, n_ii in label_word_fd[label].items():\n",
    "            n_ix = word_fd[word]\n",
    "            score = score_fn(n_ii, (n_ix, n_xi), n_xx)\n",
    "            word_scores[word] = score\n",
    "\n",
    "        bestwords = [word for word, score in word_scores.items() if score >= min_score]\n",
    "        high_info_words |= set(bestwords)\n",
    "\n",
    "    return high_info_words\n",
    "\n",
    "\n",
    "\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMINDER! : CONSIDER REMOVING STOPWORDS, STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # remove items that are not alphabetics\n",
    "    for token in tokens:\n",
    "        if token.isalpha() == False:\n",
    "            tokens.remove(token)\n",
    "    # remove punctuation and other tokens\n",
    "    for token in tokens:\n",
    "        if token in '!;@#$%^&*().,\\/?~1234567890':\n",
    "            tokens.remove(token)\n",
    "    \n",
    "    # remove urls\n",
    "    regex = re.compile(r'^[/]+')\n",
    "    for token in tokens:\n",
    "        if regex.match(token):\n",
    "            tokens.remove(token)\n",
    "    \n",
    "    bag = bag_of_words(tokens)\n",
    "    return bag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of training features\n",
    "train_feats = list ()\n",
    "for i, row in train_df.iterrows():\n",
    "    if row[2] in categories:\n",
    "        bag_cat = row[2]\n",
    "    tweet_text = row[1]\n",
    "    # tokenize tweets and put tokens in bag of words\n",
    "    bag = tokenize(tweet_text)\n",
    "    train_feats.append((bag, bag_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the high information words\n",
    "def high_information(train_feats, categories):\n",
    "    labelled_words = [(category, []) for category in categories]\n",
    "\n",
    "    # convert the formatting of our features to that required by high_information_words\n",
    "    words = defaultdict(list)\n",
    "    all_words = list()\n",
    "    for category in categories:\n",
    "        words[category] = list()\n",
    "\n",
    "    for feat in train_feats:\n",
    "        category = feat[1]\n",
    "        bag = feat[0]\n",
    "        for w in bag.keys():\n",
    "            words[category].append(w)\n",
    "            all_words.append(w)\n",
    "            #break\n",
    "\n",
    "    labelled_words = [(category, words[category]) for category in categories]\n",
    "    \n",
    "    # calculate high information words\n",
    "    # note: to adjust min occurence score add min_score = n\n",
    "    high_info_words = set(high_information_words(labelled_words,min_score = 2))\n",
    "    #print(high_info_words)\n",
    "    #high_info_words contains a list of high-information words. You may want to use only these for classification.\n",
    "    # You can restrict the words in a bag of words to be in a given 2nd list (e.g. in function read_files)\n",
    "    # e.g. bag_of_words_in_set(words, high_info_words)\n",
    "    \n",
    "    print(\"  Number of words in the data: %i\" % len(all_words))\n",
    "    print(\"  Number of distinct words in the data: %i\" % len(set(all_words)))\n",
    "    print(\"  Number of distinct 'high-information' words in the data: %i\" % len(high_info_words))\n",
    "\n",
    "    return high_info_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number of words in the data: 13428\n",
      "  Number of distinct words in the data: 3110\n",
      "  Number of distinct 'high-information' words in the data: 1392\n"
     ]
    }
   ],
   "source": [
    "high_info_words = high_information(train_feats, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains a classifier\n",
    "def train(train_feats):\n",
    "    classifier = nltk.classify.NaiveBayesClassifier.train(train_feats)\n",
    "    return classifier\n",
    "    # the following code uses the classifier with add-1 smoothing (Laplace)\n",
    "    # You may choose to use that instead\n",
    "    #from nltk.probability import LaplaceProbDist\n",
    "    #classifier = nltk.classify.NaiveBayesClassifier.train(train_feats, estimator=LaplaceProbDist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'rt': True,\n",
       "  'angelsmomaw': True,\n",
       "  'hcr': True,\n",
       "  'is': True,\n",
       "  'unwanted': True,\n",
       "  'because': True,\n",
       "  'it': True,\n",
       "  'will': True,\n",
       "  'bankrupt': True,\n",
       "  'the': True,\n",
       "  'usa': True,\n",
       "  'and': True,\n",
       "  'give': True,\n",
       "  'below': True,\n",
       "  'inferior': True,\n",
       "  'healthcare': True,\n",
       "  'for': True,\n",
       "  'all': True,\n",
       "  'gop': True,\n",
       "  'tcot': True,\n",
       "  'tweetcongress': True},\n",
       " 'negative')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = train(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.classify.naivebayes.NaiveBayesClassifier at 0x1cef5d1d160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neutral', 'negative', 'positive']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the development data\n",
    "dev_df = pd.read_csv(\"data/HCR/dev/orig/HCR_formatted_dev.csv\")\n",
    "# retrieve the possible labels from the training data\n",
    "categories = list(set(dev_df['sentiment']))\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of development features\n",
    "dev_feats = list ()\n",
    "for i, row in dev_df.iterrows():\n",
    "    if row[2] in categories:\n",
    "        bag_cat = row[2]\n",
    "    tweet_text = row[1]\n",
    "    # tokenize tweets and put tokens in bag of words\n",
    "    bag = tokenize(tweet_text)\n",
    "    dev_feats.append((bag, bag_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(classifier, test_feats, categories):\n",
    "    print (\"\\n##### Evaluation...\")\n",
    "    print(\"  Accuracy: %f\" % nltk.classify.accuracy(classifier, test_feats))\n",
    "    precisions, recalls = precision_recall(classifier, test_feats)\n",
    "    print(precisions, recalls)\n",
    "    f_measures = calculate_f(precisions, recalls)  \n",
    "\n",
    "    print(\" |-----------|-----------|-----------|-----------------|\")\n",
    "    print(\" |%-11s|%-11s|%-11s|%-11s|\" % (\"category\",\"precision\",\"recall\",\"F-measure\"))\n",
    "    print(\" |-----------|-----------|-----------|-----------------|\")\n",
    "    for category in categories:\n",
    "        if precisions[category] is None:\n",
    "            print(\" |%-11s|%-11s|%-11s|%-11s|\" % (category, \"NA\", \"NA\", \"NA\"))\n",
    "        else:\n",
    "            print(\" |%-11s|%-11f|%-11f|%-11s|\" % (category, precisions[category], recalls[category], f_measures[category]))\n",
    "    print(\" |-----------|-----------|-----------|------------------|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification import precision_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f(precisions, recalls):\n",
    "    f_measures = {}\n",
    "    #TODO calculate the f measure for each category using as input the precisions and recalls\n",
    "    for precision, recall in zip(precisions.items(), recalls.items()):\n",
    "        f_measures[precision[0]] = ((2*precision[1])*recall[1])/ (precision[1] + recall[1])\n",
    "    return f_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(classifier, testfeats):\n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "        refsets[label].add(i)\n",
    "        observed = classifier.classify(feats)\n",
    "        testsets[observed].add(i)\n",
    "    \n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "    \n",
    "    for label in classifier.labels():\n",
    "        precisions[label] = precision(refsets[label], testsets[label])\n",
    "        recalls[label] = recall(refsets[label], testsets[label])\n",
    "    \n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Evaluation...\n",
      "  Accuracy: 1.000000\n",
      "{'negative': 1.0, 'neutral': None, 'positive': None} {'negative': 1.0, 'neutral': None, 'positive': None}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-893cb3f04517>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-702821e77ba2>\u001b[0m in \u001b[0;36mevaluation\u001b[1;34m(classifier, test_feats, categories)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprecisions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecalls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision_recall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_feats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecisions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mf_measures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecisions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" |-----------|-----------|-----------|-----------------|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-0ec94aadf7d1>\u001b[0m in \u001b[0;36mcalculate_f\u001b[1;34m(precisions, recalls)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#TODO calculate the f measure for each category using as input the precisions and recalls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecisions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mf_measures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf_measures\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "evaluation(classifier, dev_feats, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
